# Manual de Instala√ß√£o e Uso do PySpark no WSL/Ubuntu

## Links √öteis
- [V√≠deo Renan Silva](https://www.youtube.com/watch?v=fQzgt0__ZJ4&ab_channel=RenanSilva)
- [Andr√© Ricardo](https://www.youtube.com/@andre_ricardo)
- [Tutorial Medium 1](https://medium.com/@salssouza/how-to-install-pyspark-in-wsl-3c4ac0e7f672)
- [Tutorial Medium 2](https://medium.com/@aitmsi/single-node-spark-pyspark-cluster-on-windows-subsystem-for-linux-wsl2-22860888a98d)
- (adicione os outros links...)

## Passos para Instala√ß√£o

### 1. Atualize o sistema
```bash
sudo apt update
sudo apt install openjdk-11-jdk python3 python3-venv wget
```

### 2. Crie um ambiente virtual Python
```bash
python3 -m venv .pyspark
source .pyspark/bin/activate
```

### 3. Baixe e instale o Apache Spark
```bash
wget https://dlcdn.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz
tar xvf spark-3.5.4-bin-hadoop3.tgz
sudo mv spark-3.5.4-bin-hadoop3 /opt/spark
```

### 4. Configure as vari√°veis de ambiente
```bash
echo "export SPARK_HOME=/opt/spark" >> ~/.profile
echo "export PATH=\$PATH:\$SPARK_HOME/bin" >> ~/.profile
export PYTHONPATH=/opt/spark/python:/opt/spark/python:
#echo "export PYTHONPATH=\$SPARK_HOME/python:\$PYTHONPATH" >> ~/.profile

source ~/.profile
```

### 5. Instale PySpark e depend√™ncias
```bash
pip install pyspark==3.5.4 py4j==0.10.9.7
#pip install py4j==0.10.9.7
```

### 6. Teste o PySpark
```bash
pyspark

from pyspark import SparkContext

import pyspark
print(pyspark.__version__)

```

## Observa√ß√µes
- Adapte comandos conforme seu ambiente.
- Consulte os links para v√≠deos e tutoriais detalhados.

---



Perfeito! üëç Para ver o Spark UI enquanto o Spark est√° rodando, voc√™ precisa garantir algumas coisas:

1Ô∏è‚É£ Rodar Spark Master e Worker

No Linux (servidor):

# inicia o Master
sudo /opt/spark/sbin/start-master.sh --host 0.0.0.0 --webui-port 8080

# inicia o Worker apontando para o Master
sudo /opt/spark/sbin/start-worker.sh spark://<IP_DO_SERVIDOR>:7077


<IP_DO_SERVIDOR> ‚Üí IP real da m√°quina Linux (ex: 192.168.0.145)

Web UI do Master: http://<IP_DO_SERVIDOR>:8080

Web UI do Worker: http://<IP_DO_SERVIDOR>:8081

2Ô∏è‚É£ Rodar PySpark com SparkUI ativo

Se voc√™ rodar PySpark no mesmo Linux:

pyspark


Enquanto o terminal estiver aberto, o driver Spark cria a porta 4040 para o Spark UI.

Voc√™ pode verificar a porta real (caso 4040 esteja ocupada) com:

sudo lsof -i :4040
sudo lsof -i :4041


Acesse no navegador:

http://<IP_DO_SERVIDOR>:4040


‚ö†Ô∏è Importante: assim que voc√™ fecha o terminal ou encerra o PySpark, o SparkUI do driver some.

3Ô∏è‚É£ Se voc√™ estiver rodando PySpark do Windows conectado ao Linux Master

No Windows:

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("RAIS") \
    .master("spark://<IP_DO_SERVIDOR>:7077") \
    .getOrCreate()


O driver vai rodar no Windows, ent√£o o Spark UI do driver fica na m√°quina Windows (porta 4040 do Windows).

Os workers continuam no Linux.

4Ô∏è‚É£ Dicas extras

Para ver SparkUI sempre dispon√≠vel, mesmo ap√≥s encerrar jobs, use History Server:

/opt/spark/sbin/start-history-server.sh


Porta padr√£o: 18080 ‚Üí http://<IP_DO_SERVIDOR>:18080

Se quiser, posso te montar um guia completo passo a passo para acessar SparkUI do Linux pelo Windows enquanto os jobs rodam, incluindo Master, Worker, driver e History Server.

Quer que eu fa√ßa isso?

Voc√™ disse:
(worker) yuan@yuan:~/datalake$ sudo lsof -i :4040
(worker) yuan@yuan:~/datalake$ sudo /opt/spark/sbin/start-master.sh --host 0.0.0.0 --webui-port 8080
starting org.apache.spark.deploy.master.Master, logging to /opt/spark/logs/spark-root-org.apache.spark.deploy.master.Master-1-yuan.out
(worker) yuan@yuan:~/datalake$ sudo /opt/spark/sbin/start-worker.sh spark://192.168.0.145:7077
starting org.apache.spark.deploy.worker.Worker, logging to /opt/spark/logs/spark-root-org.apache.spark.deploy.worker.Worker-1-yuan.out
(worker) yuan@yuan:~/datalake$ sudo lsof -i :4040
sudo lsof -i :4041
COMMAND   PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
java    15183 yuan  286u  IPv6  63013      0t0  TCP *:4041 (LISTEN)
O ChatGPT disse:

Perfeito! ‚úÖ

Isso confirma que seu Spark Driver est√° rodando e a Spark UI est√° dispon√≠vel na porta 4041.

Interpreta√ß√£o:
COMMAND   PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
java    15183 yuan  286u  IPv6  63013      0t0  TCP *:4041 (LISTEN)


PROCESSO: java ‚Üí Spark Driver

PID: 15183 ‚Üí ID do processo (pode usar para parar com kill)

PORTA: 4041 ‚Üí Spark UI do driver

STATUS: LISTEN ‚Üí ouvindo conex√µes

IPv6: usando IPv6 (* = todas as interfaces)

A porta 4040 estava ocupada por outro driver, ent√£o o Spark tentou 4041 automaticamente.

Pr√≥ximos passos:

Acessar Spark UI pelo navegador:

No mesmo Linux:

http://localhost:4041


Do Windows na mesma rede:

http://192.168.0.145:4041


Certifique-se que o firewall do Linux permite conex√µes TCP na porta 4041.

Acompanhar jobs:

No Spark UI, voc√™ ver√°: DAGs, jobs ativos, stages, tarefas, e detalhes de execu√ß√£o.

Master e Worker UI:

Master: http://192.168.0.145:8080

Worker: http://192.168.0.145:8081

Se quiser, posso te montar uma vis√£o completa de todas as portas do Spark (Master, Worker, Driver e History Server) e como acess√°-las do Windows, para n√£o ter confus√£o de portas.

Quer que eu fa√ßa isso?




















https://www.youtube.com/watch?v=fQzgt0__ZJ4&ab_channel=RenanSilva
https://www.youtube.com/@andre_ricardo
https://www.youtube.com/watch?v=LwX9FFK9ojc&ab_channel=Andr%C3%A9Ricardo
https://medium.com/@salssouza/how-to-install-pyspark-in-wsl-3c4ac0e7f672
https://medium.com/@aitmsi/single-node-spark-pyspark-cluster-on-windows-subsystem-for-linux-wsl2-22860888a98d
https://www.youtube.com/watch?v=dq183fOB1Xg&ab_channel=KeithLyons
https://www.youtube.com/watch?v=Tk-B6Nm8qOY&ab_channel=Jos%C3%A9HenriquePadovani
https://www.youtube.com/watch?v=FMeHc8JgYfA&ab_channel=BlismosAcademy
https://www.youtube.com/watch?v=xl7PgSkxPbU&ab_channel=thegreatbacana
https://www.youtube.com/watch?v=pb-hqa_-7mk&ab_channel=SureshkumarGajendran
https://www.youtube.com/watch?v=Aq5re8T1ANA&ab_channel=Webillusion

root@DESKTOP-QMQHLFF:/mnt/c/Users/PC# cd desktop
root@DESKTOP-QMQHLFF:/mnt/c/Users/PC/desktop# mkdir pyspark_linux
root@DESKTOP-QMQHLFF:/mnt/c/Users/PC/desktop# cd pyspark_linux

# Com python j√° instalado se quiser criar uma outra vers√£o dele usa-se => python3.8 -m venv myenv # Seria bom instalador o miniconda antes

root@DESKTOP-QMQHLFF:/mnt/c/Users/PC/desktop/pyspark_linux# python3 --version # Seria bom instalador o miniconda antes
# Python 3.10.12

root@DESKTOP-QMQHLFF:/mnt/c/Users/PC/desktop/pyspark_linux# python3 -m venv .pyspark # Criou-se um env ambiente com nome pyspark e escondido "."
root@DESKTOP-QMQHLFF:/mnt/c/Users/PC/desktop/pyspark_linux# source .pyspark/bin/activate  ou desativando deactivate
(.pyspark) root@DESKTOP-QMQHLFF:/mnt/c/Users/PC/desktop/pyspark_linux#

(.pyspark) root@DESKTOP-QMQHLFF:/mnt/c/Users/PC/desktop/pyspark_linux# sudo apt update or sudo apt -y update

# Instalando Java e Pyspark
# JAVA
sudo apt update
sudo apt install openjdk-11-jdk

# SPARK
https://www.apache.org/dyn/closer.lua/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz
sudo !wget https://dlcdn.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz
sudo tar xvf spark-3.5.4-bin-hadoop3.tgz # Extraindo
sudo mv spark-3.5.4-bin-hadoop3 /opt/spark # Movendo tudo para essa pasta \\wsl.localhost\Ubuntu\opt\spark


export SPARK_HOME=/opt/spark
export PATH=$PATH:$SPARK_HOME/bin
export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH # Seria bom instalador o miniconda antes


echo "export SPARK_HOME=/opt/spark" >> ~/.profile
echo "export PATH=$PATH:$SPARK_HOME/bin" >> ~/.profile
echo "export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH" >> ~/.profile

source ~/.profile
nano ~/.profile

# Se precisar
nano ~/.bashrc
source ~/.bashrc
gedit .\bashrc

sudo /opt/spark/sbin/start-master.sh --host 127.0.0.1 --webui-port 8080 # sudo /opt/spark/sbin/start-master.sh 8080
sudo /opt/spark/sbin/start-worker.sh spark://localhost:7077

sudo /opt/spark/sbin/stop-all.sh

pip install py4j
##############################################

# LEIA PARA ENTENDER A VARIAVEL DE AMBIENTE NO LINUX, SE VC ESQUECER

export SPARK_HOME=/opt/spark
export PATH=$PATH:$SPARK_HOME/bin

# export PATH=$PATH:/opt/spark/bin
# Esta linha define a vari√°vel de ambiente SPARK_HOME, 
# que informa ao sistema onde o Spark est√° instalado. 
# No caso, o Spark est√° instalado no diret√≥rio /opt/spark.
# O Spark usa essa vari√°vel para localizar seus arquivos e configura√ß√µes importantes.

### export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Users/PC/miniconda3/condabin:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0/:/mnt/c/WINDOWS/System32/OpenSSH/:/mnt/c/Program Files (x86)/PuTTY/:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/nodejs/:/Docker/host/bin:/mnt/c/Program Files/dotnet/:/mnt/c/Program Files (x86)/Terraform:/mnt/c/Users/PC/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/PC/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/PC/AppData/Roaming/npm:/mnt/c/Users/PC/AppData/Local/Programs/MiKTeX/miktex/bin/x64/:/mnt/c/Users/PC/AppData/Local/Programs/Ollama:/snap/bin:/opt/spark/bin:/opt/spark/sbin
# exemplo export PATH=$PATH:$SPARK_HOME/bin
# O PATH √© uma vari√°vel de ambiente usada pelo sistema para localizar programas execut√°veis. 
# Ao adicionar $SPARK_HOME/bin ao PATH, 
# voc√™ permite que o sistema reconhe√ßa comandos do Spark (como spark-submit e pyspark) 
# a partir de qualquer terminal


### export PYSPARK_PYTHON=/root/miniconda3/envs/ambiente/bin/python3
# export PYSPARK_PYTHON=/usr/bin/python3
# O PySpark precisa saber qual vers√£o do Python usar, e, 
#nesse caso, voc√™ est√° especificando que o Python 3 deve ser utilizado.



export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH

# 1. export:
# O comando export √© usado no Linux/Unix para definir vari√°veis de ambiente que ficam dispon√≠veis para processos filhos 
# (como scripts e programas executados em terminais).

# 2. PYTHONPATH:
# O PYTHONPATH √© uma vari√°vel de ambiente usada pelo Python para saber onde procurar os m√≥dulos (pacotes) importados.
# Quando voc√™ importa uma biblioteca no Python, o Python verifica o diret√≥rio de onde ela deve vir, 
# de acordo com os diret√≥rios listados em PYTHONPATH (junto com os diret√≥rios padr√£o do Python).

# 3. $SPARK_HOME/python:
# $SPARK_HOME √© a vari√°vel de ambiente que armazena o caminho de instala√ß√£o do Spark. 
# Ent√£o, $SPARK_HOME/python √© o diret√≥rio que cont√©m as bibliotecas Python do Spark.
# Ao usar $SPARK_HOME/python, voc√™ est√° referenciando o local onde est√£o as bibliotecas Python do Spark (como pyspark).

# 4. : $PYTHONPATH:
# O : no comando √© um separador usado para concatenar diret√≥rios no PYTHONPATH.
# No Unix/Linux, quando voc√™ deseja adicionar m√∫ltiplos diret√≥rios √† vari√°vel de ambiente PYTHONPATH, voc√™ os separa com :.
# O $PYTHONPATH no final garante que voc√™ n√£o substitua o conte√∫do anterior de PYTHONPATH, 
# mas sim que adicione o diret√≥rio $SPARK_HOME/python √† lista de diret√≥rios j√° existentes.

# This is for Jupyter notebook with Spark
# export PYSPARK_DRIVER_PYTHON=jupyter
# export PYSPARK_DRIVER_PYTHON_OPTS='notebook'
# export PYSPARK_DRIVER_PYTHON=jupyter
# export PYSPARK_DRIVER_PYTHON_OPTS='notebook'


#PYSPARK_DRIVER_PYTHON=ipython ./bin/pyspark
#PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS=notebook ./bin/pyspark


#export SPARK_HOME=/opt/spark
#export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/wsl/lib:/mnt/c/Users/PC/miniconda3/condabin:/mnt/c/WINDOWS/system32:/mnt/c/WINDOWS:/mnt/c/WINDOWS/System32/Wbem:/mnt/c/WINDOWS/System32/WindowsPowerShell/v1.0/:/mnt/c/WINDOWS/System32/OpenSSH/:/mnt/c/Program Files (x86)/PuTTY/:/mnt/c/Program Files (x86)/NVIDIA Corporation/PhysX/Common:/mnt/c/Program Files/Git/cmd:/mnt/c/Program Files/nodejs/:/Docker/host/bin:/mnt/c/Program Files/dotnet/:/mnt/c/Program Files (x86)/Terraform:/mnt/c/Users/PC/AppData/Local/Microsoft/WindowsApps:/mnt/c/Users/PC/AppData/Local/Programs/Microsoft VS Code/bin:/mnt/c/Users/PC/AppData/Roaming/npm:/mnt/c/Users/PC/AppData/Local/Programs/MiKTeX/miktex/bin/x64/:/mnt/c/Users/PC/AppData/Local/Programs/Ollama:/snap/bin:/opt/spark/bin:/opt/spark/sbin
#export PYSPARK_PYTHON=/root/miniconda3/envs/ambiente/bin/python3
#export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH

#export PYSPARK_DRIVER_PYTHON=jupyter
#export PYSPARK_DRIVER_PYTHON_OPTS="notebooks"

##############################################################################


sudo lsof -i :8080 # Procurar a porta do processo 8080
sudo kill -9 16185 # Matar o processo 16185

ps aux
kill 12345

kill -9 -1
Press Windows + R to open the Run dialog.
Type services.msc and press Enter to open the Services window.
Scroll down and find LxssManager.
Right-click on LxssManager and select Restart.
After restarting, try running wsl again





# Jeito estranho de ativar o pyspark
cd /opt/spark # Entrando na pasta
./bin/pyspark # Entrando no bin para ativar o pyspark
/opt/spark/bin/pyspark # Might be direto aqui


onde estou
pwd

cd home
ls
cd renan
cd ..
cd ~

###########
# Iceberg #
###########
spark-shell --packages org.apache.iceberg:iceberg-spark-runtime-3.1_2.12:0.13.1

from pyspark.sql import SparkSession

# Inicializar o Spark com Iceberg
spark = SparkSession.builder \
    .appName("IcebergExample") \
    .config("spark.jars.packages", "org.apache.iceberg:iceberg-spark-runtime-3.1_2.12:0.13.1") \
    .getOrCreate()





